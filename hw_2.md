# second_hw_bigdata_course_2025_FW

## Второе ДЗ по предмету BigData для потока БИВТ-22-СП | Spark, Spark Streaming, RealTime processing

Во втором ДЗ мы продолжаем отрабатывать Spark на практике, погружаемся глубже в оптимизации Spark, применяем методы оптимизаций, которые были изучены вами в курсе. Все ДЗ будет разделено на несколько блоков, максимальный балл, который можно будет получиться за все блоки равен 20. 

Что проверяем в этом модуле? Решение комплексных аналитически-инженерных задач с применением оптимизаций. Понимание и возможность реализации кастомных джойнов в Spark. Знание микробатчинга и стриминга, понимание отличий и выбор подходящего инструмента исходя из задачи. 

Важно: за курс мы смотрели с вами на множество оптимизаций. Явно описаны здесь они не будут, так как это будет прямой спойлер и очень большая подсказка. Вам нужно изучить данные, которые представлены для каждой задачи, подумать, как Spark может с ними поработать и что можно придумать. Задание с верных решением, но без оптимизаций будет принято, но балл может быть меньше, если не применена необходимая оптимизация.  

### Темы заданий в этом ДЗ:
1. RDD
2. Оптимизация Spark
3. Оптимизация Spark 
4. Spark Streaming 
5. Kafka + ClickHouse

---

### Задание 1 (максимум 4 балла)

**Что нужно сделать?**

В исходном файле представлена информация о сотрудниках, их подразделении и заработной плате.
1. Использовать Spark-сессию, которую мы использовали на семинарах или в ДЗ №1
2. Посчитать среднюю зарплату в каждом подразделении с применением Spark RDD
3. При решении задачи необходимо использовать функцию `aggregateByKey`

---

### Задание 2 (максимум 4 баллов)

**Входные данные**

1. Каталог фильмов (справочник фильмов)
2. Данные о просмотрах фильмов пользователями

**Что требуется сделать**

1. Выполнить объединение (join) основных данных о просмотрах со справочником фильмов по колонке идентификатора контента (id контента = element_uid) с использованием DataFrame API.
2. Описать возможные проблемы, которые могут возникнуть при обработке этих датасетов в Spark.
3. Решить задачу объединения любым удобным способом.
4. Решить задачу объединения с использованием оптимизаций.

---

### Задание 3 (максимум 4 баллов)

**Входные данные**

1. Таблица с длительностью просмотра контента пользователями
2. Таблица с оценками, которые пользователи поставили фильмам
3. Список (выборка) пользователей, для которых нужно рассчитать фичи

**Что требуется сделать**

1. Для каждого пользователя из выборки определить максимальное и минимальное время просмотра фильмов, которым этот пользователь поставил оценки 8, 9 или 10
2. Названия новых признаков (фичей) должны соответствовать формату, описанному в ноутбуке
3. Если у пользователя нет ни одной оценки 8, 9 или 10, то значения соответствующих фичей должны быть равны null

---

### Задание 4 (максимум 4 балла)

**Входные данные**

1. Потоковые данные о транзакциях

**Что требуется сделать**

1. Обработать потоковые данные: чтение и обработка данных о транзакциях в реальном времени, преобразовать входящие данные в DataFrame
2. Вычислить скользящее среднее значение суммы транзакций за последние 5 минут 
3. Определить аномалии: пометить проблемные транзакции, например, если значение суммы транзакции больше среднего значения суммы транзакций в 2 раза
4. Вывести данные об аномальных транзакциях в консоль в режиме реального времени

---

### Задание 5* (максимум 4 баллов)

**Входные данные**

1. Вам будет предоставлен docker-compose файл. Отличный от инфраструкты для ДЗ№1 и предыдущих заданий этого ДЗ№2
2. Генератор ивентов для топика кафки. Из этого топика нужно будет считывать данные
3. Структура данных, которые будут поступать в топик 
4. Скорипт с генерацие ивентов их записью в Kafka Topic

#### Описание входящих данных

transaction_id — уникальный идентификатор транзакции
user_id — идентификатор пользователя
timestamp — время транзакции
amount — сумма транзакции
currency — валюта транзакции
transaction_type — тип транзакции (покупка, возврат, снятие, пополнение)
status — статус транзакции (успешно, неуспешно, в ожидании)


**Что требуется сделать**

Представьте, что вам поступила задача. Необходимо по рилтайм данным собрать витрины для аналитиков, к которым они могли бы обращаться, поверх которых они могли бы строить дашборды и многое другое. 
1. Ваша задача: перенести данные из контура Kafka в контур ClickHouse
2. Построить несколько Kafka Engine таблиц (здесь на ваше усмотрение, можно одну, а можно и несколько) 
3. Учесть, что некоторые входящие данные содержать ошибки или могут не парситься и т д. Для решение этой проблемы необходимо реализовать deadletter механизм при помощи MV
4. У вас должно быть минимум две таблицы MergeTree, они должны отличаться и быть логичными, то есть содержать какой-то смысл, иными словами – быть удобными для пользователя
5. Необходимо приложить код кафка энжинов, код MV, код таблиц и скрины запросов и создания таблиц в ClickHouse

---

### Критерии оценки задач:
1. Корректность решения и валидный ответ
2. Применения оптимизаций там, где они действительно необходимы
3. Комментарии к коду и обоснование вашего решения, а не просто голый код 
4. Для задания №2 выявлена ключевая проблема: либо описана, либо скриншоты из Spark UI
5. Для задания №5 присутствует полное обоснование, аналитика по данным и скриншоты из Kafka UI, ClickHouse 
6. Логи для абсолютно всех заданий доступны в ноутбуке и могут быть прочитаны преподавателями 

*PS: вы можете приложить при отправке ДЗ не только ноутбук, но и текстовый файл в произвольной форме (не нагруженный) со скринами и решением, рассуждениями и прочим, что посчитаете необходимым при оценивании работы.*

---

### Куда отправлять выполненные задания:
- СП-1, СП-2, СП-3 (первые 16 человек по списку) на почту m2204414@edu.misis.ru
- СП-3 (вторые 16 человек по списку), СП-4, СП-5 на почту m2206664@edu.misis.ru